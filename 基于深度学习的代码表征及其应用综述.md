```[1]张祥平,刘建勋.基于深度学习的代码表征及其应用综述[J].计算机科学与探索,2022,16(09):2011-2029.```

## 一、基本知识：
#### 1.基于矩阵的表征模型-词共现矩阵
```
WxC的形状，W是词典大小，C是shiyong的上下文信息长度。通常空间要求高，所以一般会映射一段成为Wxd，d=f(C)，d<<C
```
#### 2.基于神经网络的表征模型-Word2Vec算法
```
使用Skip-gram和CBOW两个框架，完成从Wt->[Wt-2, Wt-1, Wt+1, Wt+2]->Wt的从中心词推测上下文，再从上下文推测中心词，完成词到向量的转换
```
## 二、代码的表征方式：
#### 1.基于符号序列：词法分析后的token流作为网络输入
```
(1)	使用Word2vec算法：保留基本词共现信息，缺乏逻辑信息
·	[29] C/C++的词汇单元上使用Word2vec算法用于软件漏洞的预测。其中词汇单元的表征向量用于初始化TextCNN模型进而用于代码分类。
·	[30] 计算不同代码表征向量之间的余弦相似度来找到相似的代码片段。
(2)	使用Seq2seq算法：太关注逻辑信息，丢失了结构信息
·	使用编码器（encoder）对代码中的词汇序列进行编码，得到上下文向量的 中间表示，之后使用解码器（decoder）对上下文向量进行解码，从而生成这些代码序列所对应的功能注释。其中编码器与解码器通常是使用序列神经网络作为基本的序列特征提取模型。
```
#### 2.基于API调用序列：将API调用序列作为网络输入
#### 3.基于抽象语法树：树节点与结构相关、树叶子与语义相关，树结构不能作为传统神经网络的输入，所以需要转换成序列，这个过程必然损失结构信息，但是也可开发基于树的神经网络
```
(1)	[12]词汇用RNN建模，句法用AST转为满二叉树，转为Olive Trees，使用另一个RNN建模，将两个RNN网络得出的向量结合作为代码表征
(2)	[19]提出基于树的卷积神经网络模型，采用连续二叉树的概念，直接在树上卷积，使用动态池化技术将数目不同的特征向量转换为了一个向量
(3)	[45]将API调用信息补充到AST之后，再使用基于树的卷积神经网络学习
(4)	[10]将AST切割成多个语句树，使用双向GRU学习，缓解了长依赖问题
【长依赖：一个变量的声明和使用相距很远】
```
#### 4.基于图：图节点表示元素，边表示依赖关系和数据流动，
```
(1)	[52]在AST上增加边构建图
(2)	[53]inst2vec与语言和平台无关的表征方法，针对中间形式，需要先编译
```
## 三、相关应用
#### 1.代码克隆检测
```
(1)	对代码的修改、复用、变换以及重构会使得软件系统中出现大量的相似甚至重复的代码片段。当被克隆的代码段带有缺陷时，这个缺陷将通过克隆传播
(2)	思路大多遵循：对代码片段进行预处理->对处理好的中间形式进行信息抽取，得到中间表征（深度学习网络）->根据表征方式进行相似性计算
(3)	四种克隆代码类型：Type1-完全相同的代码、Type2-重命名的代码、Type3-几乎相同的代码、Type4-语义相同的代码。1-4代码相似性降低，检测难度上升。
(4)	基于代码序列：把代码看作纯文本，通常使用后缀树匹配、最长相同序列匹配等算法比较。这种方法金策速度快，计算开销少，不依赖语言，但是只能检测1、2类型，不能检测3、4类型
(5)	基于代码结构：将代码预处理成AST之后，比较树的相似性
(6)	基于深度学习：缺乏大规模带标注的数据集，主流仍将代码看作序列
 ```
#### 2.代码搜索
```
(1)	通过自然语言的搜索，给出相应的代码段或者API。传统搜索引擎并不针对代码，所以一般无法在语义上完成自然语言和代码段的匹配。
(2)	通常是将自然语言和源代码转换为同一个向量空间的连续向量，并计算二者的相似程度。这个方法就需要代码表征足够的优秀。
(3)	[16]将两个语言映射到同一个高维度的向量空间求余弦确定相似度。这个映射是通过深度学习构建的网络完成的。这个高维向量空间是通过拆分为方法名、API、词汇单元三部分结合而来的
(4)	一般是基于序列的，也有基于图的
```
#### 3.代码补全：
```
(1)	代码补全技术将待补全位置的代码的上下文信息和代码库中学习到的代码上下文信息进行关联，通过计算两者之间的相似度，推荐相似度高的代码片段作为代码补全推荐结果。
(2)	分三类：标识符不全、代码片段不全、关键词缩略词补全
```
## 四、分析讨论
#### 1.超出此表问题OoV：
```
(1)	由于软件开发人员在编写代码时可以自由地创建标识符，这一过程将会产生一个规模巨大且稀疏的代码 词表。代码词表的规模会直接影响代码分析任务的效率。在神经网络模型训练的过程中，通常的做法是对这个大词表中的词汇单元个数进行数量的限制，因此当某个词汇在词表中没有出现过，那么神经网络模型将无法对其进行处理。
(2)	研究方向：衡量词汇单元的信息量，剔除信息量小的词汇单元；构建特征提取能力强的神经网络模型，从字符层进行信息提取
```
#### 2.表征模型的构建：
```
(1)	尚缺少对于预训练模型应用于代码分析任务的工作。现有模型都是有监督的学习，即给出一个代码段，就要给出特定的输出（深度学习不同，给一个代码段，自动给出代码表征）
(2)	研究方向：将传统的代码分析工作中代码特征提取部分融合进模型构建；增加模型训练的数据量，提高提取能力
```
#### 3.代码表征质量的评价标准
```
(1)	当前主要针对特定任务进行评价，没有通用评价方法
(2)	研究方向：统一标准并构建高质量数据集
```
```
[10] ZHANG J, WANG X, ZHANG H Y. A novel neural source code representation based on abstract syntax tree[C]//Proce edings of the 41st International Conference on Software Engineering, Montreal, May 25-31, 2019. Piscataway: IEEE, 2019: 783-794.
[12] WHITE M, TUFANO M, VENDOME C. Deep learning fragments for code clone detection[C]//Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, Singapore, Sep 3-7, 2016. New York: ACM, 2016: 87-98.
[16] GU X D, ZHANG H Y, KIM S H. Deep code search[C]// Proceedings of the 40th International Conference on Soft ware Engineering, Gothenburg, May 27- Jun 3, 2018. New York: ACM, 2018: 933-944.
[19] MOU L L, LI G, ZHANG L. Convolutional neural network over tree structures for programming language processing [C]//Proceedings of the 30th AAAI Conference on Artific ial Intelligence, Phoenix, Feb 12-17, 2016. Menlo Park: AAAI, 2016: 1287-1293.
[29] HARER J, KIM L, RUSSELL R, et al. Automated software vulnerability detection with machine learning[J]. arXiv:1803. 04497, 2018. 
[30] CHEN Z M, MONPERRUS M. The remarkable role of similarity in redundancy- based program repair[J]. arXiv:1811. 05703, 2018.
[45] CHEN L, YE W, ZHANG S K. Capturing source code semantics via tree- based convolution over API- enhanced AST[C]//Proceedings of the 16th ACM International Conf erence on Computing Frontiers, Alghero, Apr 30- May 2, 2019. New York: ACM, 2019: 174-182.
[52] BROCKSCHMIDT M, ALLAMANIS M, GAUNT A L. Generative code modeling with graphs[J]. arXiv:1805.08490, 2018.
[53] BEN- NUN T, JAKOBOVITS A S, HOEFLER T. Neural code comprehension: a learnable representation of code semantics[C]//Proceedings of the 32nd International Confe rence on Neural Information Processing Systems, Montré al, Dec 3-8, 2018: 3589-3601.
```
