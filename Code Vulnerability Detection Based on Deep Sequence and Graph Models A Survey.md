```[1]Wu B, Zou F. Code vulnerability detection based on deep sequence and graph models: A survey[J]. Security and Communication Networks, 2022, 2022.```

## 一、引入
#### 1.静态分析：
```
基于源代码，一开始是通过专家定制漏洞模式，将其定义为一个机器学习的二元分类问题，后引入深度学习模型
```
#### 2.动态分析：模糊测试
#### 3.基于序列：
```
将源代码转换为token流，尽管可能从更高层次的中间编译成果中抽取信息，但最终知识作为token流的附加信息。主要使用的深度学习网络是GRU、LSTM、Transformer。因为RNN会有梯度消失的问题，使用LSTM可以解决
```
#### 4.基于图：
```
将源代码转为抽象语法树AST、程序控制图CFG、数据流图DFG、程序依赖图PFG、代码属性图CPG，采用GNN及其变种进行学习
```
## 二、前置
#### 1.代码漏洞检测：
```
（1）代码漏洞检测被形式化为一个二元问题：检测给定的源代码是否容易受到攻击导致安全问题。
（2）可以将此转换为{（ci，yi）| ci∈C，yi∈Y}，C是源代码集合，Y是{0，1}，1表示是漏洞相关，0表示代码良好。深度学习模型的目标就是学习一个f（ci）->yi的映射，其损失函数可以定义为minΣL（f（ci，ci|yi））+l*w（f），其中L是交叉熵损失函数，w是正则化函数防止过拟合，l是正则度。ci是代码表征，表示学习模型的粒度。本文的ci要么是序列，要么是图。
```
#### 2.基于序列：
```
（1）RNN：由于梯度消失的问题，RNN通常会被其变体LSTM和GRU代替。通常，基于RNN的代码漏洞检测模型，会将源代码拆分为token流之后，传入BLSTM进行漏洞检测.
（2）Ttransformer：基于其在nlp的优秀表现，其在代码漏洞检测也有很好的应用。比如[39]的研究，其将源代码作为token嵌入和位置嵌入信息序列X，然后将其转换为向量H0，再使用N层Transformer和多头注意力机制，计算出Hn，然后应用于下游任务。基于Tranformer的BERT，指导研究者们研究一个基于代码语言和自然语言语料库的预训练模型，然后在下游任务上对模型微调，使用下游任务来验证预训练模型的优劣。
```
#### 3.基于GNN：
```
（1）使用MPNN（message passing neural network）来完成节点信息传递。
（2）研究GNN主要在于改善池化方法和聚合方法。比如GAT和GCN都是优化聚合方法，排序池化和自注意池化都是优化池化方法。
（3）基于词的漏洞检测模型，通常都使用AST、PFG、CPG等图进行表征。
```
## 三、基于序列的模型方法
#### 1.通用流程：
```
（1）源代码->规范化/切片->计算token->token转为vector->DL模型->结果
（2）源代码->计算token->token转为vector->DL模型->结果
```
 
#### 2.序列化预处理
```
（1）基于RNN：首先利用此法分析将代码拆分成若干片段，这些片段通常和由一定规则表示的漏洞代码具有数据相关或者控制相关，并且将变量和函数名规范化，从而能够解决OOV的问题。
（2）基于Transformer：一般都不会进行规范化和切片，而直接使用nlp领域的转换方法。CodeT5使用字节级别的BPE，在字节层面，不断地用新符号代码出现最频繁的相邻符号，直到字典大小在可接受范围内。CoTEXT使用SentencePieces模型进行token转换。SynCoBERT是个多模态模型，使用token流和AST的路径进行表征，token使用BPE，路径要基于词法分析。
```
#### 3.Token流转为vector
```
（1）基于word2vec：word2vec使用RNN，在CBOW（根据上下文预测中心词）或skip-gram（根据中心词预测上下文）任务上进行训练。这个向量不能包含漏洞相关信息。
（2）基于Transformer-Embedding：嵌入层在下游任务执行过程中可被调整，其输出向量更能包含漏洞信息。并且使用不同难度的任务进行预训练。
```
#### 4.学习框架：
```
（1）基于RNN：将token向量合并成一个张量放入模型学习，每一行代表一个token向量。VulDeePecker使用BLSTM进行学习，相较于LSTM，更能获取上下文的信息，在BLSTM前，使用多层dense进行降维处理，最后使用Softmax层输出概率结果。SySeVR使用BGRU进行学习。
（2）基于Transformer：使用带有768个隐藏参数和12个注意力头的十二层Tranformer进行学习。使用带有自然语言注释的代码库进行预训练，使用dense层输出分类概率。比如SynCoBERT使用标识符预测和AST边预测进行构建对比训练框架进行预训练，从而获取更多的语义信息帮助计算概率。
```
#### 5.评价：
```
（1）使用代码切片和规范化的模型，能够得到更简洁的代码token，不使用的模型可能提取到无关的上下文，从而是学习效果受到影响。
（2）基于Tranformer的向量化能够和后续任务合并联调，而是向量化的过程能够隐式地获取漏洞相关的信息，word2vec只能获得源代码的语义而获取不到漏洞相关的信息。
（3）Tranformer模型具有更好的并行性，和更大的容量。
（4）基于上述，RNN模型更关注优化代码切片和规范化的方法，而Tranformer更关注设计更优秀的学习框架。
```
## 四、基于图的模型方法
#### 1.通用流程：
```
（1）源代码(->规范化)->预处理成图->每个节点向量化->GNN学习->结果
```
#### 2.预处理成图：
```
通常在AST、CFG、PDG中挑选，也可以使用将上述三种图融合的CPG作为表征。但是CPG中含有许多与漏洞无关的代码节点，所以使用SPG，进行CPG的子图挑选，从而降低维度。
```
#### 3.向量化：
```
图中节点由代码和类型组成，代码通常由word2vec进行向量化，类型通常由单热点模式向量化，边的类型也通常使用单热点模式向量化。
```
#### 4.学习框架：
```
通常使用GGNN和R-GCN代替传统GNN，添加不同的权重策略学习。
```
#### 5.评价：
```
基于GNN的漏洞检测模型能够更好的识别语义信息，但是其规模通常较大，复杂度高。使用word2vec和one-hot进行向量化，也不能在预训练阶段隐式的学习到漏洞相关的信息。
```
## 五、数据集
#### 1.Juliet和S-Babi是根据预定义模式生成的，不合理
#### 2.Devign的数据集来自github的提交，他将所有修改的函数都标记为漏洞
#### 4.CGD、VulDeePecker、SySeVR是从NVD和SARD中搜索的，SARD细粒度标签可以直接下载，NVD需要比较diff文件
#### 5.FUNDED不能得到细粒度标签
#### 6.D2A是最好的数据集
## 六、发展方向
#### 1.代码表征：
```
基于序列忽视结构，基于图语义权重低，可以使用多模态，合并两个方法进行学习
```
#### 2.代码嵌入策略：
```
代码在向量化过程中，通常使用word2vec，但是word2vec不能和后续检测网络联调，所以在向量化过程会漏掉漏洞信息
```
#### 3.模型选择：
```
基于序列的模型忽视了控制流等关系，基于GNN在学习语义方面表现不够好，在GNN的图基础加上NCS（自然代码序列）也有距离过长问题，可以使对两个网络框架进行融合，提升性能
```
#### 4.数据集:
```
当前数据集不够统一，性能表现的可信度不够高，且数据集粒度较粗
```
#### 5.细粒度模型:
```
当前模型主要的粒度集中在函数级别，无法确定是哪些语句导致漏洞
```
