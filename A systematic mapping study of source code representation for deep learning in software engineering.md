```[1]Samoaa H P, Bayram F, Salza P, et al. A systematic mapping study of source code representation for deep learning in software engineering[J]. IET Software, 2022, 16(4): 351-385.```

## 一、前置研究
#### 1.基于记号的表征：
```
1）	词嵌入（word embedding）：将每个记号通过概率模型映射到预先设定好的向量空间。最终相似的词得到向量也是相似的。这只能学习单个词语，而不能学习上下文的关系。
2）	N-Grams：联系上下文对单词映射到向量中。能够学习上下文信息。
```
#### 2.基于树的表征：
```
1）	AST能够展示词法信息和语法结构信息，但是AST的节点没有标点和分隔符
2）	流行的TNN：RvNN、TBCNN、Tree-LSTM
```
#### 3.基于图
```
1)	相较于树，图能展示更多的语义消息，但是语法结构被弱化
2)	CFG，控制流图：表示语句块的控制权的流动，如函数调用、分支、循环等
3)	DFG，数据流图：表示变量和语句被引用的关系，有向边表示变量被语句引用
4)	PDG，程序依赖图：将控制流和数据流同时表现在一张图上
```
## 二、深度学习在软件工程的应用概述
#### 1.高层概括模型：
```
1）	数据收集：根据源码收集并进行标注成数据集
2）	数据预处理：代码变为代码表征、向量化、特征学习。文中提出了集中图嵌入技术——graph2vec、HOPE、SDNE、node2vec
3）	模型训练和验证：训练集训练模型，测试集验证模型
```
## 三、数据分析
#### 1.软件工程任务区分：
```
1)	Code-Code：使用DL模型构建code到code的映射，通常是代码克隆检测、代码相似性检测和程序修复任务
2)	Code-Text：使用DL模型构建code到自然语言的映射，通常是注释生成、标识符命名建议等任务
3)	Text-Code：使用DL模型构建自然文本到code的映射，通常是代码检索、代码生成等任务
4)	Code-Prediction：使用DL模型构建code到概率的映射，通常是缺陷、漏洞、恶意行为检测
```
#### 2.使用的DL模型：LSTM、注意力机制、CNN、编码-解码器、RNN、ANN、CNN。其中CNN+注意力机制构成了Transformer，LSTM能解决RNN梯度消失的问题、ANN和GNN的输入不是序列的，注意力机制可以应用到多种网络中提升学习质量
#### 3.下游任务对DL的选择：
```
1)	Text-Code/Code-Text：主要使用LSTM、注意力机制和编码器
2)	Code-Code：主要使用LSTM
3)	Code-Prediction：主要使用LSTM和CNN，GNN用的很少
```
#### 4.任务同表征的选择：
```
1）	基于图、树：Code-Prediction和Code-Code占比多
2）	基于记号：Code-Text和Text-Code最适合基于token表示
```
5.	基于图的具体下游任务
```
1)	Code-Code：代码克隆检测和代码相似性检测使用图多
2)	Code-Prediction：漏洞检测、bug检测使用图多
```
## 四、混合任务框架
#### 1.略
## 五、混合代码表征
#### 1.通常来说，树和记号结合多，图和树、记号结合相差不大
## 六、研究缺乏
#### 1.研究覆盖：
```
现在研究的下游任务缺乏性能预测、代码异味检测、可追溯性
```
#### 2.缺乏通用性：
```
可使用迁移学习和预训练模型来消除代码表征和下游任务两个模型学习的隔离性
```
#### 3.缺乏统一的数据集
## 七、评价与讨论
#### 1.基于AST的深度学习：
```
相较于记号，AST包含更加丰富的结构信息，但是其缺乏语义信息，且树的深度过深时，会导致梯度消失的问题
```
#### 2.基于图的深度学习：
```
图有更丰富的语义信息，但是其蕴含的语法信息消失了部分，并且CFG没有数据流动信息，而DFG在某些语言很难得出，且空间复杂度大。但是使用图作为下游任务学习是更合适的
```